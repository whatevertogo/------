{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e34162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import requests\n",
    "\n",
    "# 设置随机种子\n",
    "#torch.manual_seed(42)\n",
    "#np.random.seed(42)\n",
    "\n",
    "# 检查CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    return str(timedelta(seconds=int(seconds)))\n",
    "\n",
    "def check_internet_connection():\n",
    "    try:\n",
    "        requests.get(\"https://huggingface.co\", timeout=5)\n",
    "        print(\"网络连接正常。\")\n",
    "    except requests.ConnectionError:\n",
    "        print(\"网络连接失败，请检查网络。\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67997b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义collate_fn，将InputExample对象转换为适合模型输入的格式。\n",
    "    \"\"\"\n",
    "    sentences1 = [example.texts[0] for example in batch]\n",
    "    sentences2 = [example.texts[1] for example in batch]\n",
    "    labels = [example.label for example in batch]\n",
    "    return {\n",
    "        'sentences1': sentences1,\n",
    "        'sentences2': sentences2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "class SentencePairDataset:\n",
    "    def __init__(self, sentences1, sentences2, labels):\n",
    "        self.examples = [InputExample(texts=[s1, s2], label=float(label))\n",
    "                         for s1, s2, label in zip(sentences1, sentences2, labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def evaluate(model, data_loader, device, threshold=0.84):\n",
    "    \"\"\"\n",
    "    高效评估函数，批量编码所有句子对并计算相似度。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings1 = []\n",
    "    all_embeddings2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 批量编码所有句子对\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sentences1 = batch['sentences1']\n",
    "            sentences2 = batch['sentences2']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "            all_embeddings1.append(embeddings1)\n",
    "            all_embeddings2.append(embeddings2)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # 将所有嵌入拼接为单个张量\n",
    "    all_embeddings1 = torch.cat(all_embeddings1, dim=0)\n",
    "    all_embeddings2 = torch.cat(all_embeddings2, dim=0)\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.int, device=device)\n",
    "\n",
    "    # 一次性计算余弦相似度\n",
    "    similarity = torch.cosine_similarity(all_embeddings1, all_embeddings2, dim=1)\n",
    "    preds = (similarity > threshold).cpu().numpy().astype(int)\n",
    "\n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels.cpu().numpy(), preds)\n",
    "    f1 = f1_score(all_labels.cpu().numpy(), preds)\n",
    "    recall = recall_score(all_labels.cpu().numpy(), preds)\n",
    "    score = 0.6 * f1 + 0.2 * accuracy + 0.2 * recall\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "def train_model(model, train_data, val_data, device, batch_size=64, epochs=3, patience=3, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    完全重写的训练函数，包含学习率调度器\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    print(f\"模型是否在GPU上: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    print(\"准备训练数据集...\")\n",
    "    train_dataset = SentencePairDataset(\n",
    "        train_data['q1'].tolist(),\n",
    "        train_data['q2'].tolist(),\n",
    "        train_data['label'].tolist()\n",
    "    )\n",
    "    print(\"准备验证数据集...\")\n",
    "    val_dataset = SentencePairDataset(\n",
    "        val_data['q1'].tolist(),\n",
    "        val_data['q2'].tolist(),\n",
    "        val_data['label'].tolist()\n",
    "    )\n",
    "    \n",
    "    # 数据加载器\n",
    "    print(\"创建数据加载器...\")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # Windows下使用0避免多进程问题\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    # 优化器和损失函数\n",
    "    print(\"初始化优化器和损失函数...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    # 添加学习率调度器\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, \n",
    "                                verbose=True, min_lr=1e-6)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    best_score = 0\n",
    "    no_improve_epochs = 0\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(\"开始训练循环...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        try:\n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "                # 获取数据\n",
    "                sentences1 = batch['sentences1']\n",
    "                sentences2 = batch['sentences2']\n",
    "                labels = torch.tensor(batch['labels'], dtype=torch.float32, device=device)\n",
    "\n",
    "                # 清除梯度\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 将句子转换为模型输入格式\n",
    "                features1 = model.tokenize(sentences1)\n",
    "                features2 = model.tokenize(sentences2)\n",
    "\n",
    "                # 将输入移动到设备上\n",
    "                features1 = {key: val.to(device) for key, val in features1.items()}\n",
    "                features2 = {key: val.to(device) for key, val in features2.items()}\n",
    "\n",
    "                # 计算余弦相似度损失\n",
    "                loss = train_loss([features1, features2], labels)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 更新进度条\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # 进行验证\n",
    "            val_metrics = evaluate(model, val_dataloader, device)\n",
    "            current_score = val_metrics['score']\n",
    "            \n",
    "            # 更新学习率\n",
    "            scheduler.step(current_score)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            print(f\"当前学习率: {current_lr:.2e}\")\n",
    "            print(f\"验证集评估: Accuracy={val_metrics['accuracy']:.4f}, \"\n",
    "                  f\"F1={val_metrics['f1']:.4f}, Recall={val_metrics['recall']:.4f}\")\n",
    "            print(f\"当前得分: {current_score:.4f}\")\n",
    "            \n",
    "            # 保存每个epoch的模型\n",
    "            epoch_save_path = f'model_epoch_{epoch+1}'\n",
    "            print(f\"保存当前epoch模型到: {epoch_save_path}\")\n",
    "            model.save(epoch_save_path)\n",
    "            \n",
    "            # 检查是否需要保存最佳模型\n",
    "            if current_score > best_score + min_delta:\n",
    "                best_score = current_score\n",
    "                no_improve_epochs = 0\n",
    "                print(f\"保存最佳模型，得分: {best_score:.4f}\")\n",
    "                model.save('best_model')\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"模型表现未提升，已经 {no_improve_epochs}/{patience} 个epoch\")\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(\"\\nEarly stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"训练过程中出错: {e}\")\n",
    "            progress_bar.close()\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a645d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据...\n",
      "训练集大小: 214889\n",
      "验证集大小: 23877\n",
      "测试集大小: 4401\n",
      "\n",
      "训练集类别分布:\n",
      "label\n",
      "1    124672\n",
      "0     90217\n",
      "Name: count, dtype: int64\n",
      "网络连接正常。\n",
      "\n",
      "加载预训练模型...\n",
      "预训练模型加载成功！\n",
      "\n",
      "开始训练...\n",
      "模型是否在GPU上: cuda:0\n",
      "准备训练数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备验证数据集...\n",
      "创建数据加载器...\n",
      "初始化优化器和损失函数...\n",
      "开始训练循环...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   2%|▏         | 80/3358 [12:51<10:54:26, 11.98s/it, loss=0.1094]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m模型已保存为: chinese_semantic_model_final\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 4. 训练模型\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m开始训练...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 可以根据GPU显存调整\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m               \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 5. 加载最佳模型进行测试\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m加载最佳模型进行测试...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_data, val_data, device, batch_size, epochs, patience, min_delta)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[32m    150\u001b[39m loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# 更新进度条\u001b[39;00m\n\u001b[32m    154\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:634\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    631\u001b[39m     scaled_device_grads = device_grads  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    632\u001b[39m     value = \u001b[32m1\u001b[39m - beta2\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_addcmul_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_device_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# Delete the local intermediate(s) since they won't be used anymore to save on peak memory\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m device_grads\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. 加载数据\n",
    "    print(\"加载数据...\")\n",
    "    data = pd.read_csv('csv/train.tsv', sep='\\t', header=None, names=['q1', 'q2', 'label'])\n",
    "    test_data = pd.read_csv('csv/test.csv', sep='\\t', header=None, names=['q1', 'q2', 'label'])\n",
    "\n",
    "    # 拆分训练集和验证集\n",
    "    train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "\n",
    "    print(f\"训练集大小: {len(train_data)}\")\n",
    "    print(f\"验证集大小: {len(val_data)}\")\n",
    "    print(f\"测试集大小: {len(test_data)}\")\n",
    "    print(\"\\n训练集类别分布:\")\n",
    "    print(train_data['label'].value_counts())\n",
    "\n",
    "    # 2. 检查网络连接\n",
    "    if not check_internet_connection():\n",
    "        return\n",
    "\n",
    "    # 3. 加载模型#model = SentenceTransformer('bge-large-zh-v1.5')\n",
    "    print(\"\\n加载预训练模型...\")\n",
    "    try:\n",
    "        model = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
    "        print(\"预训练模型加载成功！\")\n",
    "    except Exception as e:\n",
    "        print(f\"预训练模型加载失败: {e}\")\n",
    "        print(\"请检查网络连接或模型名称是否正确。\")\n",
    "        return\n",
    "\n",
    "    # 4. 训练模型\n",
    "    print(\"\\n开始训练...\")\n",
    "    model = train_model(model, train_data, val_data, device, \n",
    "                   batch_size=64,  # 可以根据GPU显存调整\n",
    "                   epochs=3,\n",
    "                   patience=3)\n",
    "\n",
    "    # 5. 加载最佳模型进行测试\n",
    "    print(\"\\n加载最佳模型进行测试...\")\n",
    "    try:\n",
    "        model = SentenceTransformer('best_model')\n",
    "        print(\"模型加载成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"模型加载失败: {e}\")\n",
    "        return\n",
    "\n",
    "    # 6. 准备测试集\n",
    "    print(\"准备测试集...\")\n",
    "    test_dataset = SentencePairDataset(\n",
    "        test_data['q1'].tolist(),\n",
    "        test_data['q2'].tolist(),\n",
    "        test_data['label'].tolist()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=96, \n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # 不使用多进程\n",
    "        collate_fn=custom_collate_fn  # 使用自定义的collate_fn\n",
    "    )\n",
    "\n",
    "    # 7. 最终测试\n",
    "    print(\"进行最终测试...\")\n",
    "    try:\n",
    "        test_metrics = evaluate(model, test_loader, device)\n",
    "        print(\"测试完成\")\n",
    "    except Exception as e:\n",
    "        print(f\"测试失败: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n最终测试结果:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"最终得分: {test_metrics['score']:.4f}\")\n",
    "\n",
    "    # 8. 保存最终模型\n",
    "    model.save('chinese_semantic_model_final')\n",
    "    print(\"\\n模型已保存为: chinese_semantic_model_final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce3c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "加载测试数据...\n",
      "测试集大小: 4401\n",
      "\n",
      "加载模型 best_model...\n",
      "模型加载成功\n",
      "模型是否在GPU上: cuda:0\n",
      "准备测试集...\n",
      "进行测试评估...\n",
      "\n",
      "测试结果:\n",
      "Accuracy: 0.8603\n",
      "F1-score: 0.8510\n",
      "Recall: 0.7850\n",
      "最终得分: 0.8396\n",
      "\n",
      "尝试不同阈值:\n",
      "阈值=0.840: Accuracy=0.8603, F1=0.8510, Recall=0.7850, Score=0.8396\n",
      "阈值=0.845: Accuracy=0.8573, F1=0.8470, Recall=0.7769, Score=0.8350\n",
      "阈值=0.850: Accuracy=0.8564, F1=0.8456, Recall=0.7734, Score=0.8333\n",
      "阈值=0.855: Accuracy=0.8564, F1=0.8450, Recall=0.7702, Score=0.8323\n",
      "阈值=0.860: Accuracy=0.8532, F1=0.8410, Recall=0.7635, Score=0.8279\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义collate_fn，将InputExample对象转换为适合模型输入的格式。\n",
    "    \"\"\"\n",
    "    sentences1 = [example.texts[0] for example in batch]\n",
    "    sentences2 = [example.texts[1] for example in batch]\n",
    "    labels = [example.label for example in batch]\n",
    "    return {\n",
    "        'sentences1': sentences1,\n",
    "        'sentences2': sentences2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "class SentencePairDataset:\n",
    "    def __init__(self, sentences1, sentences2, labels):\n",
    "        self.examples = [InputExample(texts=[s1, s2], label=float(label))\n",
    "                         for s1, s2, label in zip(sentences1, sentences2, labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def evaluate(model, data_loader, device, threshold=0.84):\n",
    "    \"\"\"\n",
    "    高效评估函数，批量编码所有句子对并计算相似度。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings1 = []\n",
    "    all_embeddings2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 批量编码所有句子对\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sentences1 = batch['sentences1']\n",
    "            sentences2 = batch['sentences2']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "            all_embeddings1.append(embeddings1)\n",
    "            all_embeddings2.append(embeddings2)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # 将所有嵌入拼接为单个张量\n",
    "    all_embeddings1 = torch.cat(all_embeddings1, dim=0)\n",
    "    all_embeddings2 = torch.cat(all_embeddings2, dim=0)\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.int, device=device)\n",
    "\n",
    "    # 一次性计算余弦相似度\n",
    "    similarity = torch.cosine_similarity(all_embeddings1, all_embeddings2, dim=1)\n",
    "    preds = (similarity > threshold).cpu().numpy().astype(int)\n",
    "\n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels.cpu().numpy(), preds)\n",
    "    f1 = f1_score(all_labels.cpu().numpy(), preds)\n",
    "    recall = recall_score(all_labels.cpu().numpy(), preds)\n",
    "    score = 0.6 * f1 + 0.2 * accuracy + 0.2 * recall\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'score': score,\n",
    "        'similarity': similarity.cpu().numpy(),  # 返回相似度分数用于进一步分析\n",
    "        'labels': all_labels.cpu().numpy()       # 返回真实标签用于进一步分析\n",
    "    }\n",
    "\n",
    "def test_best_model(test_file='csv/test.csv', model_path='best_model', batch_size=96, threshold=0.84):\n",
    "    \"\"\"\n",
    "    加载最佳模型并在测试集上验证性能\n",
    "    \n",
    "    Args:\n",
    "        test_file: 测试集文件路径\n",
    "        model_path: 模型保存路径\n",
    "        batch_size: 批处理大小\n",
    "        threshold: 相似度阈值，用于确定样本是否属于同一类\n",
    "        \n",
    "    Returns:\n",
    "        dict: 包含测试指标的字典\n",
    "    \"\"\"\n",
    "    # 检查CUDA\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 1. 加载测试数据\n",
    "    print(\"加载测试数据...\")\n",
    "    test_data = pd.read_csv(test_file, sep='\\t', header=None, names=['q1', 'q2', 'label'])\n",
    "    print(f\"测试集大小: {len(test_data)}\")\n",
    "    \n",
    "    # 2. 加载最佳模型\n",
    "    print(f\"\\n加载模型 {model_path}...\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model.to(device)\n",
    "        print(\"模型加载成功\")\n",
    "        print(f\"模型是否在GPU上: {next(model.parameters()).device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"模型加载失败: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # 3. 准备测试集\n",
    "    print(\"准备测试集...\")\n",
    "    test_dataset = SentencePairDataset(\n",
    "        test_data['q1'].tolist(),\n",
    "        test_data['q2'].tolist(),\n",
    "        test_data['label'].tolist()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # 不使用多进程\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    # 4. 进行测试评估\n",
    "    print(\"进行测试评估...\")\n",
    "    test_metrics = evaluate(model, test_loader, device, threshold)\n",
    "    \n",
    "    # 5. 输出结果\n",
    "    print(\"\\n测试结果:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"最终得分: {test_metrics['score']:.4f}\")\n",
    "    \n",
    "    # 6. 分析不同阈值的结果\n",
    "    print(\"\\n尝试不同阈值:\")\n",
    "    for th in [0.84, 0.845, 0.85, 0.855, 0.86]:\n",
    "        preds = (test_metrics['similarity'] > th).astype(int)\n",
    "        acc = accuracy_score(test_metrics['labels'], preds)\n",
    "        f1 = f1_score(test_metrics['labels'], preds)\n",
    "        recall = recall_score(test_metrics['labels'], preds)\n",
    "        score = 0.6 * f1 + 0.2 * acc + 0.2 * recall\n",
    "        print(f\"阈值={th:.3f}: Accuracy={acc:.4f}, F1={f1:.4f}, Recall={recall:.4f}, Score={score:.4f}\")\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行测试\n",
    "    test_metrics = test_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41b54d",
   "metadata": {},
   "source": [
    "threshold=0.84  0.8947 0.8358"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c05a9",
   "metadata": {},
   "source": [
    "threshold=0.85 0.8929 0.8359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ca9bc",
   "metadata": {},
   "source": [
    "threshold=0.855 0.8868 0.8369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2102a63",
   "metadata": {},
   "source": [
    "threshold=0.86 0.8858 0.8350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c951a4",
   "metadata": {},
   "source": [
    "threshold=0.845 0.8893 0.8351"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
