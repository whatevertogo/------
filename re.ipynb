{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e34162",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import requests\n",
    "\n",
    "# 设置随机种子\n",
    "#torch.manual_seed(42)\n",
    "#np.random.seed(42)\n",
    "\n",
    "# 检查CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    return str(timedelta(seconds=int(seconds)))\n",
    "\n",
    "def check_internet_connection():\n",
    "    try:\n",
    "        requests.get(\"https://huggingface.co\", timeout=5)\n",
    "        print(\"网络连接正常。\")\n",
    "    except requests.ConnectionError:\n",
    "        print(\"网络连接失败，请检查网络。\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67997b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义collate_fn，将InputExample对象转换为适合模型输入的格式。\n",
    "    \"\"\"\n",
    "    sentences1 = [example.texts[0] for example in batch]\n",
    "    sentences2 = [example.texts[1] for example in batch]\n",
    "    labels = [example.label for example in batch]\n",
    "    return {\n",
    "        'sentences1': sentences1,\n",
    "        'sentences2': sentences2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "class SentencePairDataset:\n",
    "    def __init__(self, sentences1, sentences2, labels):\n",
    "        self.examples = [InputExample(texts=[s1, s2], label=float(label))\n",
    "                         for s1, s2, label in zip(sentences1, sentences2, labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def evaluate(model, data_loader, device, threshold=0.84):\n",
    "    \"\"\"\n",
    "    高效评估函数，批量编码所有句子对并计算相似度。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings1 = []\n",
    "    all_embeddings2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 批量编码所有句子对\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sentences1 = batch['sentences1']\n",
    "            sentences2 = batch['sentences2']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "            all_embeddings1.append(embeddings1)\n",
    "            all_embeddings2.append(embeddings2)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # 将所有嵌入拼接为单个张量\n",
    "    all_embeddings1 = torch.cat(all_embeddings1, dim=0)\n",
    "    all_embeddings2 = torch.cat(all_embeddings2, dim=0)\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.int, device=device)\n",
    "\n",
    "    # 一次性计算余弦相似度\n",
    "    similarity = torch.cosine_similarity(all_embeddings1, all_embeddings2, dim=1)\n",
    "    preds = (similarity > threshold).cpu().numpy().astype(int)\n",
    "\n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels.cpu().numpy(), preds)\n",
    "    f1 = f1_score(all_labels.cpu().numpy(), preds)\n",
    "    recall = recall_score(all_labels.cpu().numpy(), preds)\n",
    "    score = 0.6 * f1 + 0.2 * accuracy + 0.2 * recall\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "def train_model(model, train_data, val_data, device, batch_size=64, epochs=3, patience=3, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    完全重写的训练函数，包含学习率调度器\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    print(f\"模型是否在GPU上: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    print(\"准备训练数据集...\")\n",
    "    train_dataset = SentencePairDataset(\n",
    "        train_data['q1'].tolist(),\n",
    "        train_data['q2'].tolist(),\n",
    "        train_data['label'].tolist()\n",
    "    )\n",
    "    print(\"准备验证数据集...\")\n",
    "    val_dataset = SentencePairDataset(\n",
    "        val_data['q1'].tolist(),\n",
    "        val_data['q2'].tolist(),\n",
    "        val_data['label'].tolist()\n",
    "    )\n",
    "    \n",
    "    # 数据加载器\n",
    "    print(\"创建数据加载器...\")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # Windows下使用0避免多进程问题\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    # 优化器和损失函数\n",
    "    print(\"初始化优化器和损失函数...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    # 添加学习率调度器\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, \n",
    "                                verbose=True, min_lr=1e-6)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    best_score = 0\n",
    "    no_improve_epochs = 0\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(\"开始训练循环...\")\n",
    "    # 优化训练循环，加入自动混合精度\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            sentences1 = batch['sentences1']\n",
    "            sentences2 = batch['sentences2']\n",
    "            labels = torch.tensor(batch['labels'], dtype=torch.float32, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                # 将句子转换为模型输入格式\n",
    "                features1 = model.tokenize(sentences1)\n",
    "                features2 = model.tokenize(sentences2)\n",
    "\n",
    "                # 将输入移动到设备上\n",
    "                features1 = {key: val.to(device) for key, val in features1.items()}\n",
    "                features2 = {key: val.to(device) for key, val in features2.items()}\n",
    "\n",
    "                # 计算余弦相似度损失\n",
    "                loss = train_loss([features1, features2], labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        progress_bar.close()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # 进行验证\n",
    "        val_metrics = evaluate(model, val_dataloader, device)\n",
    "        current_score = val_metrics['score']\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\"平均训练损失: {avg_loss:.4f}\")\n",
    "        print(f\"验证集评估: Accuracy={val_metrics['accuracy']:.4f}, F1={val_metrics['f1']:.4f}, Recall={val_metrics['recall']:.4f}\")\n",
    "        print(f\"当前得分: {current_score:.4f}\")\n",
    "\n",
    "        # 保存每个epoch的模型\n",
    "        epoch_save_path = f'model_epoch_{epoch+1}'\n",
    "        print(f\"保存当前epoch模型到: {epoch_save_path}\")\n",
    "        model.save(epoch_save_path)\n",
    "\n",
    "        # 检查是否需要保存最佳模型\n",
    "        if current_score > best_score + min_delta:\n",
    "            best_score = current_score\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"保存最佳模型，得分: {best_score:.4f}\")\n",
    "            model.save('best_model')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"模型表现未提升，已经 {no_improve_epochs}/{patience} 个epoch\")\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(\"\\nEarly stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce3c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "加载测试数据...\n",
      "测试集大小: 4401\n",
      "\n",
      "加载模型 best_model...\n",
      "模型加载成功\n",
      "模型是否在GPU上: cuda:0\n",
      "准备测试集...\n",
      "进行测试评估...\n",
      "\n",
      "测试结果:\n",
      "Accuracy: 0.8280\n",
      "F1-score: 0.8095\n",
      "Recall: 0.7188\n",
      "最终得分: 0.7950\n",
      "\n",
      "尝试不同阈值:\n",
      "阈值=0.840: Accuracy=0.8280, F1=0.8095, Recall=0.7188, Score=0.7950\n",
      "阈值=0.845: Accuracy=0.8248, F1=0.8047, Recall=0.7099, Score=0.7897\n",
      "阈值=0.850: Accuracy=0.8232, F1=0.8020, Recall=0.7045, Score=0.7868\n",
      "阈值=0.855: Accuracy=0.8203, F1=0.7977, Recall=0.6974, Score=0.7822\n",
      "阈值=0.860: Accuracy=0.8180, F1=0.7944, Recall=0.6916, Score=0.7785\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义collate_fn，将InputExample对象转换为适合模型输入的格式。\n",
    "    \"\"\"\n",
    "    sentences1 = [example.texts[0] for example in batch]\n",
    "    sentences2 = [example.texts[1] for example in batch]\n",
    "    labels = [example.label for example in batch]\n",
    "    return {\n",
    "        'sentences1': sentences1,\n",
    "        'sentences2': sentences2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "class SentencePairDataset:\n",
    "    def __init__(self, sentences1, sentences2, labels):\n",
    "        self.examples = [InputExample(texts=[s1, s2], label=float(label))\n",
    "                         for s1, s2, label in zip(sentences1, sentences2, labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def evaluate(model, data_loader, device, threshold=0.84):\n",
    "    \"\"\"\n",
    "    高效评估函数，批量编码所有句子对并计算相似度。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings1 = []\n",
    "    all_embeddings2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 批量编码所有句子对\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sentences1 = batch['sentences1']\n",
    "            sentences2 = batch['sentences2']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "            all_embeddings1.append(embeddings1)\n",
    "            all_embeddings2.append(embeddings2)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # 将所有嵌入拼接为单个张量\n",
    "    all_embeddings1 = torch.cat(all_embeddings1, dim=0)\n",
    "    all_embeddings2 = torch.cat(all_embeddings2, dim=0)\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.int, device=device)\n",
    "\n",
    "    # 一次性计算余弦相似度\n",
    "    similarity = torch.cosine_similarity(all_embeddings1, all_embeddings2, dim=1)\n",
    "    preds = (similarity > threshold).cpu().numpy().astype(int)\n",
    "\n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels.cpu().numpy(), preds)\n",
    "    f1 = f1_score(all_labels.cpu().numpy(), preds)\n",
    "    recall = recall_score(all_labels.cpu().numpy(), preds)\n",
    "    score = 0.6 * f1 + 0.2 * accuracy + 0.2 * recall\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'score': score,\n",
    "        'similarity': similarity.cpu().numpy(),  # 返回相似度分数用于进一步分析\n",
    "        'labels': all_labels.cpu().numpy()       # 返回真实标签用于进一步分析\n",
    "    }\n",
    "\n",
    "def test_best_model(test_file='csv/test.csv', model_path='best_model', batch_size=64, threshold=0.84):\n",
    "    \"\"\"\n",
    "    加载最佳模型并在测试集上验证性能\n",
    "    \n",
    "    Args:\n",
    "        test_file: 测试集文件路径\n",
    "        model_path: 模型保存路径\n",
    "        batch_size: 批处理大小\n",
    "        threshold: 相似度阈值，用于确定样本是否属于同一类\n",
    "        \n",
    "    Returns:\n",
    "        dict: 包含测试指标的字典\n",
    "    \"\"\"\n",
    "    # 检查CUDA\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 1. 加载测试数据\n",
    "    print(\"加载测试数据...\")\n",
    "    test_data = pd.read_csv(test_file, sep='\\t', header=None, names=['q1', 'q2', 'label'])\n",
    "    print(f\"测试集大小: {len(test_data)}\")\n",
    "    \n",
    "    # 2. 加载最佳模型\n",
    "    print(f\"\\n加载模型 {model_path}...\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model.to(device)\n",
    "        print(\"模型加载成功\")\n",
    "        print(f\"模型是否在GPU上: {next(model.parameters()).device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"模型加载失败: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 3. 准备测试集\n",
    "    print(\"准备测试集...\")\n",
    "    test_dataset = SentencePairDataset(\n",
    "        test_data['q1'].tolist(),\n",
    "        test_data['q2'].tolist(),\n",
    "        test_data['label'].tolist()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # 不使用多进程\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    # 4. 进行测试评估\n",
    "    print(\"进行测试评估...\")\n",
    "    test_metrics = evaluate(model, test_loader, device, threshold)\n",
    "    \n",
    "    # 5. 输出结果\n",
    "    print(\"\\n测试结果:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"最终得分: {test_metrics['score']:.4f}\")\n",
    "    \n",
    "    # 6. 分析不同阈值的结果\n",
    "    print(\"\\n尝试不同阈值:\")\n",
    "    for th in [0.84, 0.845, 0.85, 0.855, 0.86]:\n",
    "        preds = (test_metrics['similarity'] > th).astype(int)\n",
    "        acc = accuracy_score(test_metrics['labels'], preds)\n",
    "        f1 = f1_score(test_metrics['labels'], preds)\n",
    "        recall = recall_score(test_metrics['labels'], preds)\n",
    "        score = 0.6 * f1 + 0.2 * acc + 0.2 * recall\n",
    "        print(f\"阈值={th:.3f}: Accuracy={acc:.4f}, F1={f1:.4f}, Recall={recall:.4f}, Score={score:.4f}\")\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行测试\n",
    "    test_metrics = test_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41b54d",
   "metadata": {},
   "source": [
    "threshold=0.84  0.8947 0.8358"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c05a9",
   "metadata": {},
   "source": [
    "threshold=0.85 0.8929 0.8359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ca9bc",
   "metadata": {},
   "source": [
    "threshold=0.855 0.8868 0.8369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2102a63",
   "metadata": {},
   "source": [
    "threshold=0.86 0.8858 0.8350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c951a4",
   "metadata": {},
   "source": [
    "threshold=0.845 0.8893 0.8351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc4e9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据...\n",
      "训练集大小: 214889\n",
      "验证集大小: 23877\n",
      "测试集大小: 4401\n",
      "\n",
      "训练集类别分布:\n",
      "label\n",
      "1    124695\n",
      "0     90194\n",
      "Name: count, dtype: int64\n",
      "训练集大小: 214889\n",
      "验证集大小: 23877\n",
      "测试集大小: 4401\n",
      "\n",
      "训练集类别分布:\n",
      "label\n",
      "1    124695\n",
      "0     90194\n",
      "Name: count, dtype: int64\n",
      "网络连接正常。\n",
      "\n",
      "加载预训练模型...\n",
      "网络连接正常。\n",
      "\n",
      "加载预训练模型...\n",
      "预训练模型加载成功！\n",
      "\n",
      "开始训练...\n",
      "模型是否在GPU上: cuda:0\n",
      "准备训练数据集...\n",
      "预训练模型加载成功！\n",
      "\n",
      "开始训练...\n",
      "模型是否在GPU上: cuda:0\n",
      "准备训练数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18794\\Desktop\\比赛\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备验证数据集...\n",
      "创建数据加载器...\n",
      "初始化优化器和损失函数...\n",
      "开始训练循环...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 6716/6716 [25:34<00:00,  4.82it/s, loss=0.0661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "当前学习率: 2.00e-05\n",
      "验证集评估: Accuracy=0.8868, F1=0.8957, Recall=0.8364\n",
      "当前得分: 0.8821\n",
      "保存当前epoch模型到: model_epoch_1\n",
      "保存最佳模型，得分: 0.8821\n",
      "保存最佳模型，得分: 0.8821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 6716/6716 [26:17<00:00,  4.26it/s, loss=0.0661]\u001b[A\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m模型已保存为: chinese_semantic_model_final\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mmain_v3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain_v3\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 4. 训练模型（减少训练轮次和批处理大小）\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m开始训练...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 减小批次大小以提高泛化性\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 减少训练轮次\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m               \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 降低patience以更快响应性能下降\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 提高阈值以确保显著改进\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 5. 加载最佳模型进行测试\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m加载最佳模型进行测试...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_data, val_data, device, batch_size, epochs, patience, min_delta)\u001b[39m\n\u001b[32m    151\u001b[39m optimizer.step()\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# 更新进度条\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m progress_bar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    156\u001b[39m progress_bar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m})\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main_v3():\n",
    "    # 1. 加载数据\n",
    "    print(\"加载数据...\")\n",
    "    data = pd.read_csv('csv/train.tsv', sep='\\t', header=None, names=['q1', 'q2', 'label'])\n",
    "    test_data = pd.read_csv('csv/test.csv', sep='\\t', header=None, names=['q1', 'q2', 'label'])\n",
    "\n",
    "    # 拆分训练集和验证集 (添加随机种子确保可复现)\n",
    "    train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "    print(f\"训练集大小: {len(train_data)}\")\n",
    "    print(f\"验证集大小: {len(val_data)}\")\n",
    "    print(f\"测试集大小: {len(test_data)}\")\n",
    "    print(\"\\n训练集类别分布:\")\n",
    "    print(train_data['label'].value_counts())\n",
    "\n",
    "    # 2. 检查网络连接\n",
    "    if not check_internet_connection():\n",
    "        return\n",
    "\n",
    "    # 3. 加载模型\n",
    "    print(\"\\n加载预训练模型...\")\n",
    "    try:\n",
    "        model = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
    "        print(\"预训练模型加载成功！\")\n",
    "    except Exception as e:\n",
    "        print(f\"预训练模型加载失败: {e}\")\n",
    "        print(\"请检查网络连接或模型名称是否正确。\")\n",
    "        return\n",
    "\n",
    "    # 4. 训练模型（减少训练轮次和批处理大小）\n",
    "    print(\"\\n开始训练...\")\n",
    "    model = train_model(model, train_data, val_data, device, \n",
    "                   batch_size=32,     # 减小批次大小以提高泛化性\n",
    "                   epochs=5,          # 减少训练轮次\n",
    "                   patience=2,        # 降低patience以更快响应性能下降\n",
    "                   min_delta=1e-3)    # 提高阈值以确保显著改进\n",
    "\n",
    "    # 5. 加载最佳模型进行测试\n",
    "    print(\"\\n加载最佳模型进行测试...\")\n",
    "    try:\n",
    "        model = SentenceTransformer('best_model')\n",
    "        print(\"模型加载成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"模型加载失败: {e}\")\n",
    "        return\n",
    "\n",
    "    # 6. 准备测试集（保持与训练时相同的batch_size）\n",
    "    print(\"准备测试集...\")\n",
    "    test_dataset = SentencePairDataset(\n",
    "        test_data['q1'].tolist(),\n",
    "        test_data['q2'].tolist(),\n",
    "        test_data['label'].tolist()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=32,  # 与训练时保持一致\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    # 7. 最终测试\n",
    "    print(\"进行最终测试...\")\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "\n",
    "    print(\"\\n最终测试结果:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"最终得分: {test_metrics['score']:.4f}\")\n",
    "\n",
    "    # 8. 保存最终模型\n",
    "    model.save('chinese_semantic_model_final')\n",
    "    print(\"\\n模型已保存为: chinese_semantic_model_final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_v3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
